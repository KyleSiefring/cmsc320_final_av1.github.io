{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSERT TITLE HERE\n",
    "\n",
    "In the AV1 video coding format, a new entropy coder was added. An entropy\n",
    "encoder is responsible for compressing the instructions that allow\n",
    "reconstructing the video frame.\n",
    "\n",
    "The advantage of the new entropy coder is that it uses a parallel method\n",
    "capable of decoding more than two possibilities in one step. In comparison,\n",
    "previous coders used a serial process that decoded in a binary manner. This was\n",
    "a substantial bottleneck for hardware, which are able to utilize fine-grained\n",
    "parallelization to a much greater extent than cpus or gpus. It's also possible\n",
    "to write SIMD (TODO: link) for this process, so there can also be some benefits\n",
    "for cpus.\n",
    "\n",
    "To take advantage of this parallelism, data must be encoded from an 'alphabet'\n",
    "of more than 2 'letters' or 'symbol'. The largest supported alphabet size is 16,\n",
    "which allows processing 4-bit long letters. Each alphabet has an associated\n",
    "distribution for each letter, $i$, stored and operated on in the form\n",
    "$1 - CDF_i$ (in fixed-point arithmetic). This is an approximate of the frequency\n",
    "that takes spatial locality into account instead of an exact model of frequency\n",
    "like huffman coding. This structure for storing distributions will be referred\n",
    "to as a CDF.\n",
    "\n",
    "CDFs are generally updated whenever a symbol is encoded. Some CDFs are\n",
    "hardcoded in the code, and there is also functionality to turn off updates\n",
    "some period of time.\n",
    "\n",
    "Frames can be split into 'tiles' with independent CDF 'contexts'. The tiles\n",
    "in the next frame get their contexts from previous contexts or, alternatively,\n",
    "resets to the initial cdfs.\n",
    "\n",
    "In practice, AV1 doesn't take advantage of larger alphabets as much as one\n",
    "would hope. In this tutorial, we will take a file containing a dump of a video's\n",
    "symbol data and analyze the performance of the entropy encoder. We won't be\n",
    "considering any video data that disable updates to CDFs or any of the hardcoded\n",
    "CDFs.\n",
    "\n",
    "TODO: Links https://jmvalin.ca/daala/revisiting/\n",
    "\n",
    "## Getting the Data\n",
    "\n",
    "libaom v3.1.0 was used to encode a clip using the following options\n",
    "``aomenc -o <output>.ivf <input> --profile=0 --cpu-used=1 --threads=2 --tile-columns=2 --end-usage=q --cq-level=43``.\n",
    "See ``aomenc --help`` for what these options do.\n",
    "\n",
    "The clip used is ``Netflix_ToddlerFountain_4096x2160_60fps_10bit_420.y4m``\n",
    "(can be found [here](https://media.xiph.org/video/derf/)) that has been\n",
    "converted to 8 bits and downsampled to 1080p using ffmpeg.\n",
    "\n",
    "To dump symbol data, modifications were made to the decoder in libaom. A branch\n",
    "containing those changes can be found here (TODO: Link). Building the encoder\n",
    "is broken on this branch, so it is necessary to run ``make aomdec`` instead of\n",
    "just ``make`` to build successfully. The first 600 out of 1200 frames were\n",
    "decoded using ``../aomdec <input>.ivf -o /dev/null --progress -t 1 --limit=600``.\n",
    "\n",
    "The resulting output can be found here (TODO: Link)\n",
    "\n",
    "### Format\n",
    "The file contains a list of events and data for each event.\n",
    "\n",
    "Events:\n",
    "- The initial cdf and some descriptors for the event. Descriptors include:\n",
    "    - An id to uniquely identify the individual cdf.\n",
    "    - The function name of the CDF, or a custom name already provided by libaom.$^*$\n",
    "    - src file and line number that the symbol first encoded on.$^*$\n",
    "- Encode a symbol with update.\n",
    "- Encode a symbol without update (not supported).\n",
    "- Move CDFs from one context to another.\n",
    "-\n",
    "\n",
    "The\n",
    "\n",
    "$^*$ Unfortunately, it might be possible for these to be different for one id\n",
    "with two different files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%load_ext Cython\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "# Tree that demonstrates the movement of contexts.\n",
    "cdef class ContextTree:\n",
    "    # Starting context or the\n",
    "    cdef public int root_context\n",
    "    # dict mapping to children that copy from a given context\n",
    "    # list of  = dict[parent]\n",
    "    cdef public dict children\n",
    "    cdef public dict parents\n",
    "\n",
    "    def __init__(self):\n",
    "        self.root_context = 0\n",
    "        self.children = {self.root_context:[]}\n",
    "        self.parents = {}\n",
    "\n",
    "    def chain_contexts(self, parent, child):\n",
    "        self.parents[child] = parent\n",
    "        self.children[parent].append(child)\n",
    "        self.children.setdefault(child, [])\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "from libc.stdio cimport FILE, fopen, fclose, fgetc, fread, feof\n",
    "\n",
    "# Read little endian 16-bit unsigned integer\n",
    "cdef inline int read_u16_le(unsigned char* buf):\n",
    "    return buf[0] | (buf[1] << 8)\n",
    "\n",
    "cdef inline str fread_str(FILE* file, unsigned char* buf):\n",
    "    fread(buf, 1, 1, file)\n",
    "    strlen = buf[0]\n",
    "    fread(buf, 1, strlen, file)\n",
    "    return buf[:strlen].decode()\n",
    "\n",
    "def cython_load_symbol_file(char* filename):\n",
    "    cdef int event_type\n",
    "    cdef char symbol\n",
    "    cdef unsigned char buf[255]\n",
    "    cdef FILE* file = fopen(filename, 'rb')\n",
    "\n",
    "    cdef list df_name_id = []\n",
    "    cdef dict symbol_data = {}\n",
    "    cdef dict initial_cdfs = {}\n",
    "    cdef context_tree = ContextTree()\n",
    "\n",
    "    cdef dict current_context = None\n",
    "\n",
    "    # Cache the current cdf for extra performance\n",
    "    # TODO: change name\n",
    "    cdef int current_cdf_id = -1\n",
    "    cdef bytearray current_cdf_symbols = None\n",
    "\n",
    "    # Loop through each event,\n",
    "    while True:\n",
    "        event_type = fgetc(file)\n",
    "        if event_type == -1: # EOF\n",
    "            break\n",
    "        elif event_type == 0: # initialize defaults\n",
    "            fread(buf, 1, 3, file)\n",
    "            (cdf_id, num_symbols) = read_u16_le(buf), buf[2]\n",
    "            name = fread_str(file, buf)\n",
    "            #fread(buf, 1, strlen, file)\n",
    "            #name = buf[:strlen].decode()\n",
    "            src_file = fread_str(file, buf)\n",
    "            fread(buf, 1, 2, file)\n",
    "            src_line = read_u16_le(buf)\n",
    "            # Remove full path\n",
    "            src_file = src_file[src_file.rfind('/')+1:]\n",
    "            df_name_id.append([cdf_id, name, src_file, src_line])\n",
    "\n",
    "            initial_cdf = np.zeros(num_symbols, np.uint16)\n",
    "            for i in range(num_symbols):\n",
    "                fread(buf, 1, 2, file)\n",
    "                initial_cdf[i] = read_u16_le(buf)\n",
    "            initial_cdfs[cdf_id] = initial_cdf\n",
    "        elif event_type == 1:\n",
    "            # read symbol with a context update\n",
    "            fread(buf, 1, 3, file)\n",
    "            symbol, cdf_id = buf[0], read_u16_le(buf + 1)\n",
    "            if cdf_id != current_cdf_id:\n",
    "                current_cdf_id = cdf_id\n",
    "                current_cdf_symbols = current_context.setdefault(cdf_id, bytearray())\n",
    "            current_cdf_symbols.append(symbol)\n",
    "        elif event_type == 2:\n",
    "            # read symbol without updating context\n",
    "            print('Symbols encoded without cdf update not supported')\n",
    "        elif event_type == 3:\n",
    "            # copy a previous context into this one\n",
    "            fread(buf, 1, 4, file)\n",
    "            parent_context_timestamp = read_u16_le(buf)\n",
    "            child_context_timestamp = read_u16_le(buf + 2)\n",
    "            context_tree.chain_contexts(\n",
    "                parent_context_timestamp, child_context_timestamp\n",
    "            )\n",
    "        elif event_type == 4:\n",
    "            # change the current context\n",
    "\n",
    "            # read what\n",
    "            fread(buf, 1, 2, file)\n",
    "            context_timestamp = read_u16_le(buf)\n",
    "\n",
    "            # This will probably never reuse a context\n",
    "            current_context = symbol_data.setdefault(context_timestamp, {})\n",
    "\n",
    "            # Invalidate the cdf cache\n",
    "            current_cdf_id = -1\n",
    "            current_cdf_symbols = None\n",
    "        else:\n",
    "            print('Unrecognized event read: %d'% event_type)\n",
    "            break\n",
    "    fclose(file)\n",
    "\n",
    "    return symbol_data, context_tree, df_name_id, initial_cdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_symbol_file(filename):\n",
    "    symbol_data, context_tree, df_name_id, initial_cdfs = cython_load_symbol_file(filename)\n",
    "    #size = 0\n",
    "    for x in symbol_data.values():\n",
    "        for (a, b) in x.items():\n",
    "            x[a] = np.array(b, dtype=np.uint8)\n",
    "            # size+=len(x[a])\n",
    "    #np.savez('tmp.npz', symbol_data)\n",
    "    #symbol_data = np.load('tmp.npz', allow_pickle=True)\n",
    "    # tmp = open(\"tmp.pickle\", \"wb\") # TODO: stop writing to ssd\n",
    "    # pickle.dump(symbol_data, tmp)\n",
    "    # tmp.close()\n",
    "    #\n",
    "    # # Defragment memory???\n",
    "    # tmp = open(\"tmp.pickle\", \"rb\")\n",
    "    # symbol_data = pickle.load(tmp)\n",
    "    # tmp.close()\n",
    "\n",
    "    df_cdfs = pd.DataFrame(\n",
    "        df_name_id,\n",
    "        columns=['id', 'descriptor', 'src_file', 'line']\n",
    "    )\n",
    "    # TODO: Join for each sample.\n",
    "\n",
    "    return symbol_data, context_tree, df_cdfs, initial_cdfs\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# TODO: rename df_name_id\n",
    "symbol_data, context_tree, df_name_id, initial_cdfs = load_symbol_file(b\"/home/kyle/Programming/VideoEncode/libaom/events.bin\")\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print('duration', end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "#def symbol_bits(np.ndarray[np.uint16_t, ndim=1] cdf, int symbol):\n",
    "#    cdf[symbol]\n",
    "#    pass\n",
    "\n",
    "# Holds an initial count variable\n",
    "cdef class CDFState:\n",
    "    cdef public np.ndarray cdf\n",
    "    # Number of symbols encoded so far\n",
    "    cdef public int count\n",
    "\n",
    "    cdef int min_prob\n",
    "    cdef float base_prob\n",
    "\n",
    "    def __init__(self, cdf, count = 0):\n",
    "        self.cdf = cdf.copy()\n",
    "        self.count = count\n",
    "\n",
    "        self.min_prob = 4\n",
    "        # TODO: Should be close to right, need to verify\n",
    "        self.base_prob = np.log2((1<<15) + self.min_prob*len(cdf))\n",
    "\n",
    "    # def transfer(self):\n",
    "    #     # In av1, using a previous context always resets the count\n",
    "    #     return CDFState(self.cdf, count=0)\n",
    "\n",
    "    # See https://github.com/xiph/rav1e/blob/c5bc3402475bf6b61907cfc0d73574caaaee3be9/src/ec.rs#L894\n",
    "    def update(self, int symbol):\n",
    "        cdef np.ndarray[np.uint16_t, ndim=1] cdf = self.cdf\n",
    "        cdef int nsymbols = len(cdf)\n",
    "        cdef int rate = 3 + min((nsymbols + 1) >> 1, 2)\n",
    "\n",
    "        rate += (self.count >> 4)\n",
    "        self.count += 1 - (self.count >> 5)\n",
    "\n",
    "        for i in range(nsymbols):\n",
    "            if i >= symbol:\n",
    "                cdf[i] -= cdf[i] >> rate\n",
    "            else:\n",
    "                cdf[i] += ((1 << 15) - cdf[i]) >> rate\n",
    "\n",
    "    def calc_symbol_cost(self, int symbol):\n",
    "        cdef np.ndarray[np.uint16_t, ndim=1] cdf = self.cdf\n",
    "        a = 1<<15\n",
    "        if symbol != 0:\n",
    "            a = int(cdf[symbol - 1])\n",
    "        diff = a - cdf[symbol]\n",
    "        return self.base_prob - np.log2(float(diff + self.min_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import scipy.signal\n",
    "import scipy.ndimage\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "def calc_symbol_cost(cdf_state, symbol):\n",
    "    nsymbols = len(cdf_state.cdf)\n",
    "    a = 1<<15\n",
    "    if symbol != 0:\n",
    "        a = int(cdf_state.cdf[symbol - 1])\n",
    "    diff = a - cdf_state.cdf[symbol]\n",
    "    min_prob = 4\n",
    "    # TODO: Should be close to right, need to verify\n",
    "    return np.log2(\n",
    "        ((1<<15) + min_prob*nsymbols)/(diff + min_prob)\n",
    "    )\n",
    "\n",
    "def data_entropy(data):\n",
    "    return scipy.stats.entropy(\n",
    "        np.unique(data,return_counts=True,axis=0)[1], axis=0, base=2\n",
    "    )\n",
    "\n",
    "class SymbolCounters:\n",
    "    def __init__(self, size):\n",
    "        self.counts = np.zeros(size)\n",
    "\n",
    "    def count_symbols(self, symbols):\n",
    "        #indices, counts_scatter = np.unique(symbols, return_counts=True)\n",
    "        #self.counts[indices] += counts_scatter\n",
    "        self.counts += np.bincount(symbols, minlength=len(self.counts))\n",
    "\n",
    "    def entropy(self):\n",
    "        return scipy.stats.entropy(self.counts, base=2)\n",
    "\n",
    "    def to_ec_cdf(self):\n",
    "        cdf = 1 - (self.counts / self.counts.sum()).cumsum()\n",
    "        return (cdf * (1<<15)).astype(np.uint16)\n",
    "\n",
    "def iter_contexts(tree):\n",
    "    path = [tree.root_context]\n",
    "    while len(path) != 0:\n",
    "        parent = path.pop()\n",
    "        for child in tree.children[parent]:\n",
    "            #symbols = symbol_data.get(child) # TODO: purge empty???\n",
    "            #if symbols is not None:\n",
    "            yield parent, child\n",
    "            path.append(child)\n",
    "\n",
    "#for p, c in iter_contexts(context_tree):\n",
    "#    print(p, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "df_stats = pd.DataFrame(\n",
    "    columns=[\n",
    "        'id',\n",
    "        'descriptor',\n",
    "        'src_file',\n",
    "        'line',\n",
    "        'alphabet_size',\n",
    "        'bits',\n",
    "        'num_symbols',\n",
    "        'bits_per_symbol',\n",
    "        'bits_over_entropy',\n",
    "        #'bits_saved_vs_entropy'\n",
    "    ]\n",
    ")\n",
    "\n",
    "total_cost = 0\n",
    "total_entropy = 0\n",
    "context_end_cdfs = {context_tree.root_context: initial_cdfs}\n",
    "for cdf_id, initial_cdf in initial_cdfs.items():\n",
    "    new_row = dict()\n",
    "    # TODO: JUST COPY FROM TABLE\n",
    "    new_row['id'] = cdf_id\n",
    "    new_row['descriptor'] = df_name_id[df_name_id.id == cdf_id].descriptor.tolist()[0]\n",
    "    new_row['src_file'] = df_name_id[df_name_id.id == cdf_id].src_file.tolist()[0]\n",
    "    new_row['line'] = df_name_id[df_name_id.id == cdf_id].line.tolist()[0]\n",
    "    new_row['alphabet_size'] = len(initial_cdf)\n",
    "    num_symbols = 0\n",
    "    cost = 0\n",
    "    count = SymbolCounters(len(initial_cdf))\n",
    "\n",
    "    for parent_context, child_context in iter_contexts(context_tree):\n",
    "        cdf = context_end_cdfs[parent_context][cdf_id]\n",
    "        context_end_cdfs.setdefault(child_context, {})[cdf_id] = cdf\n",
    "        context = symbol_data[child_context]\n",
    "        if context is None:\n",
    "            continue\n",
    "        #if child_context != 1:\n",
    "        #    continue\n",
    "\n",
    "        arr = context.get(cdf_id)\n",
    "        if arr is not None:# and len(arr) > 100:# and cdf_id == 2520:\n",
    "            num_symbols += len(arr)\n",
    "\n",
    "            cdf_state = CDFState(cdf)\n",
    "\n",
    "            count.count_symbols(arr)\n",
    "            # static_matrix = np.repeat(count.counts[:,None], len(cdf), axis=1)\n",
    "            # static_matrix = static_matrix @ static_matrix.T\n",
    "            # static_matrix /= static_matrix.sum()\n",
    "\n",
    "\n",
    "            #test_cdf = count.to_ec_cdf()\n",
    "            #cdf_state = CDFState(test_cdf, count=32)\n",
    "\n",
    "\n",
    "            # Quasi-Markov Chain\n",
    "            # tmp = arr.view()[0:len(arr) >> 1 << 1]\n",
    "            # tmp = tmp.reshape((len(tmp) >> 1, 2))\n",
    "            # indices, counts_scatter = np.unique(tmp, return_counts=True, axis=0)\n",
    "            #print(len(initial_cdf))\n",
    "            #print(scipy.stats.entropy(counts_scatter, base=2)/2)\n",
    "\n",
    "            # matrix = np.zeros((len(cdf), len(cdf)))\n",
    "            # matrix[indices[:,0], indices[:,1]] += counts_scatter\n",
    "            # matrix /= matrix.sum() / (1 << 15)\n",
    "\n",
    "\n",
    "            cdf_time_series = []\n",
    "            for symbol in arr:\n",
    "                #cost += calc_symbol_cost(cdf_state, symbol)\n",
    "                cost += cdf_state.calc_symbol_cost(symbol)\n",
    "                cdf_state.update(symbol)\n",
    "                #cdf_time_series.append(cdf_state.cdf.copy())\n",
    "            context_end_cdfs[child_context][cdf_id] = cdf_state.cdf\n",
    "            #print(cdf_state.cdf)\n",
    "\n",
    "            # if len(cdf) <= 4:\n",
    "            #     cdf = -np.diff(cdf, prepend=(1<<15)).astype(float)\n",
    "            #     static_matrix = np.repeat(cdf[:,None], len(cdf), axis=1)\n",
    "            #     static_matrix = (static_matrix @ static_matrix.T).flatten()\n",
    "            #     static_matrix /= static_matrix.sum()\n",
    "            #     cdf = static_matrix\n",
    "            #     cdf = ((1 - cdf.cumsum()) * (1<<15)).astype(np.uint16)\n",
    "            #\n",
    "            #     tmp = tmp @ np.array([1,len(cdf)])\n",
    "            #\n",
    "            #     cdf_state = CDFState(cdf)\n",
    "            #     costW = 0\n",
    "            #     for symbol in tmp:\n",
    "            #         costW += calc_symbol_cost(cdf_state, symbol)\n",
    "            #         cdf_state.update(symbol)\n",
    "            #     print(costW/len(arr))\n",
    "                #print(cdf)\n",
    "\n",
    "\n",
    "            # cdf_time_series = np.array(cdf_time_series, dtype=float)\n",
    "            # #cdf_time_series -= cdf_time_series.mean(axis=0)\n",
    "            # cdf_time_series = -np.diff(cdf_time_series, axis=1, prepend=(1<<15))\n",
    "            # cdf_time_series = cdf_time_series.T\n",
    "\n",
    "\n",
    "            # cdf_time_series = scipy.signal.convolve2d(\n",
    "            #     cdf_time_series,\n",
    "            #     np.ones((1,64))/64,\n",
    "            #     mode='valid'\n",
    "            # )\n",
    "            # cdf_time_series = scipy.ndimage.gaussian_filter1d(\n",
    "            #     cdf_time_series,\n",
    "            #     sigma = 15,\n",
    "            #     axis=1\n",
    "            # )\n",
    "\n",
    "\n",
    "            # adfs = []\n",
    "            # adf_pvals = []\n",
    "            # for series in cdf_time_series:\n",
    "            #     adf, pval = adfuller(series)[:2]\n",
    "            #     adfs.append(adf)\n",
    "            #     adf_pvals.append(pval)\n",
    "                #print(adf, pval)\n",
    "                #print(adfuller(series, regresults=True)[-1].resols.summary())\n",
    "                # if not np.isnan(adf):\n",
    "                #     kpss_stat, pvalue = kpss(series, nlags='auto')[:2]\n",
    "                #     print(kpss_stat, pvalue)\n",
    "\n",
    "            #print(df_name_id[df_name_id.id == cdf_id].descriptor.tolist())\n",
    "            # print('ID: %5d'%cdf_id+'\\tLevels: %d'%len(cdf_state.cdf)+\n",
    "            #       '\\tbits: %.2f'% cost+'\\tsymbols: %d'% len(arr),\n",
    "            #       '\\tbits/symbol: %.2f'%avg_cost+\n",
    "            #       ' entropy: %.2f'%entropy+\n",
    "            #       ' diff: %.2f'%entropy_diff)\n",
    "            #print()\n",
    "    if num_symbols == 0:\n",
    "        continue\n",
    "    new_row['num_symbols'] = num_symbols\n",
    "\n",
    "    total_cost += cost\n",
    "    new_row['bits'] = cost # bit cost???\n",
    "    avg_cost = cost/num_symbols\n",
    "    new_row['bits_per_symbol'] = avg_cost\n",
    "\n",
    "    entropy = count.entropy()\n",
    "    new_row['entropy'] = entropy\n",
    "\n",
    "    #entropy_diff = entropy - avg_cost\n",
    "    #bits_saved = entropy_diff * num_symbols\n",
    "    #new_row['bits_saved_vs_entropy'] = bits_saved\n",
    "\n",
    "    entropy_ratio = np.nan\n",
    "    if entropy != 0:\n",
    "        entropy_ratio = avg_cost / entropy\n",
    "    new_row['bits_over_entropy'] = entropy_ratio\n",
    "\n",
    "    total_entropy += entropy*num_symbols\n",
    "\n",
    "    df_stats = df_stats.append(new_row, ignore_index=True)\n",
    "\n",
    "# TODO: Might not want to display bits_per_symbol. Nah. Keep it.\n",
    "# TODO: Rename this monstrosity avg_cost/bits_over_alphabet_size\n",
    "df_stats['bits_per_symbol_over_alphabet_size'] = df_stats.bits_per_symbol / df_stats.alphabet_size\n",
    "\n",
    "print(total_cost)\n",
    "print(total_entropy)\n",
    "end_time = datetime.datetime.now()\n",
    "print('duration', end_time - start_time)\n",
    "\n",
    "# plt.plot(np.arange(cdf_time_series.shape[1]), cdf_time_series[0])\n",
    "# plt.plot(np.arange(cdf_time_series.shape[1]), cdf_time_series[1])\n",
    "# plt.plot(np.arange(cdf_time_series.shape[1]), cdf_time_series[2])\n",
    "# plt.plot(np.arange(cdf_time_series.shape[1]), cdf_time_series[3])\n",
    "#plt.show()\n",
    "\n",
    "#df_stats.sort_values('bits_saved_vs_entropy')[:50]\n",
    "#df_stats[[x[0][0] == 'read_mv_component' for x in df_stats.descriptors]].sort_values('bits', ascending=False)[:35]\n",
    "#df_stats.sort_values('bits_per_choice', ascending=False)[0:25]\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "df_stats.sort_values('bits', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lots of duplicate of src + line. Want some concept of what is happening with\n",
    "them combined. DropNA???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Use as a check\n",
    "\n",
    "# TODO: fix me earlier\n",
    "#df_stats.bits_per_symbol_over_alphabet_size = df_stats.bits_per_symbol_over_alphabet_size.apply(pd.to_numeric)\n",
    "#df_stats.alphabet_size = df_stats.alphabet_size.apply(pd.to_numeric)\n",
    "\n",
    "#group_stats = df_stats.dropna().groupby(['descriptor', 'src_file', 'line'])\n",
    "group_stats = df_stats.groupby(['descriptor', 'src_file', 'line'])\n",
    "combined = group_stats.agg(\n",
    "    count = ('id', 'count'),\n",
    "    bits = ('bits', 'sum'),\n",
    "    num_symbols = ('num_symbols', 'sum'),\n",
    "    # Weighted average\n",
    "    bits_per_symbol_over_alphabet_size = (\n",
    "            'bits_per_symbol_over_alphabet_size',\n",
    "            lambda x: np.average(x, weights=df_stats.loc[x.index, \"num_symbols\"])\n",
    "        )\n",
    ").reset_index()\n",
    "combined.sort_values(by='bits', ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined.sort_values(by='bits', ascending=False)[0:25]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined.sort_values(by='bits_per_symbol_over_alphabet_size', ascending=False)[0:25]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined.filter(regex='read_mv', axis = 0)\n",
    "combined[combined.descriptor.str.match('read_mv')]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mv_bits_stats = df_stats[df_stats.descriptor.str.match('read_mv') & (df_stats.line == 858)][\n",
    "    [\"id\", \"bits\", \"num_symbols\", \"bits_per_symbol_over_alphabet_size\"]\n",
    "]\n",
    "mv_bits_stats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There should be enough symbols to form a reasonable confidence level for all of these."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.swarmplot(x=mv_bits_stats.bits_per_symbol_over_alphabet_size)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Move up earlier\n",
    "#sns.swarmplot(\n",
    "sns.violinplot(\n",
    "    x=df_stats[df_stats.num_symbols > 30].bits_per_symbol_over_alphabet_size,\n",
    "#    x=df_stats[df_stats.num_symbols > 50].bits_per_symbol_over_alphabet_size,\n",
    "#    size=2\n",
    "    cut=0\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "#df_stats[df_stats.num_symbols > 30].bits.to_numpy()\n",
    "\n",
    "#df_stats[df_stats.num_symbols > 30].alphabet_size.to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#subset = df_stats.filter()\n",
    "\n",
    "# Too many items\n",
    "df_stats.sort_values('bits_per_symbol_over_alphabet_size', ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# group_stats.filter(lambda x: len(x) > 15).boxplot(\n",
    "#     by=['descriptor', 'src_file', 'line'],\n",
    "#     column='bits_per_symbol_over_alphabet_size',\n",
    "#     #rot,\n",
    "#     vert = False\n",
    "# )\n",
    "# plt.show()\n",
    "\n",
    "#group_stats.filter(lambda x: len(x) > 100)\n",
    "\n",
    "#combined.sort_values('count', ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Filter out entries with small cost or few symbols.\n",
    "\n",
    "Maybe do work on\n",
    "\n",
    "TODO: expand"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df_stats[df_stats.num_symbols > 200].sort_values(\n",
    "#     'bits_per_symbol_over_alphabet_size',\n",
    "#     ascending=False\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df_stats[\n",
    "#     df_stats.num_symbols > 100\n",
    "# ].sort_values(\n",
    "#     'bits_per_symbol_over_alphabet_size',\n",
    "#     #'bits_over_entropy',\n",
    "#     ascending=False\n",
    "# ).to_csv(\"stats.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test = df_stats[['src_file', 'line']]\n",
    "# test = test.value_counts()\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heights = df_stats[df_stats.bits > 1000].sort_values(\n",
    "    'bits_over_entropy', ascending=False\n",
    ").bits_over_entropy.to_numpy()\n",
    "print(len(heights))\n",
    "\n",
    "plt.bar(\n",
    "    np.arange(len(heights)),\n",
    "    heights-1,\n",
    "    bottom=1\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# plt.plot(np.arange(cdf_time_series.shape[1]), cdf_time_series[0])\n",
    "# plt.plot(np.arange(cdf_time_series.shape[1]), cdf_time_series[1])\n",
    "# plt.plot(np.arange(cdf_time_series.shape[1]), cdf_time_series[2])\n",
    "# plt.plot(np.arange(cdf_time_series.shape[1]), cdf_time_series[3])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: 100% should use seperate columns for line/src\n",
    "# TODO: ditch\n",
    "id_joint = 4055\n",
    "id_mvsign0 = 4060\n",
    "id_mvsign1 = 4078\n",
    "id_mvclass0 = 4056\n",
    "id_mvclass1 = 4074\n",
    "joint_symbols = context[id_joint]\n",
    "mvsign0 = context[id_mvsign0]\n",
    "mvsign1 = context[id_mvsign1]\n",
    "mvclass0 = context[id_mvclass0]\n",
    "mvclass1 = context[id_mvclass1]\n",
    "print(np.bincount(joint_symbols))\n",
    "print(len(mvsign0))\n",
    "print(len(mvsign1))\n",
    "mv0_index = 0\n",
    "mv1_index = 0\n",
    "matrix = []\n",
    "for x in joint_symbols:\n",
    "    if x == 0:\n",
    "        matrix.append([0, 0])\n",
    "    elif x == 1:\n",
    "        matrix.append([0, mvclass1[mv1_index] + 1])\n",
    "        mv1_index += 1\n",
    "    elif x == 2:\n",
    "        #matrix.append([mvclass0[mv0_index] > 1, 0])\n",
    "        matrix.append([mvclass1[mv0_index] + 1, 0])\n",
    "        mv0_index += 1\n",
    "    elif x == 3:\n",
    "        #matrix.append([mvclass0[mv0_index] > 1, 1])\n",
    "        matrix.append([mvclass0[mv0_index] + 1, mvclass1[mv1_index] + 1])\n",
    "        mv0_index += 1\n",
    "        mv1_index += 1\n",
    "#print(mvclass0)\n",
    "print(\n",
    "    len(joint_symbols) * data_entropy(joint_symbols) +\n",
    "    len(mvclass0) * data_entropy(np.minimum(mvclass0,2)) +\n",
    "    len(mvclass1) * data_entropy(np.minimum(mvclass1,2)))\n",
    "print(len(matrix) * data_entropy(np.minimum(np.array(matrix), 3)))\n",
    "\n",
    "#print(df_name_id.id.unique().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#adf, pvalue = adfuller((np.random.random(1000)-.5).cumsum())[:2]\n",
    "#print(adf, pvalue)\n",
    "\n",
    "# gen = (np.random.random(10000) - 0.5).cumsum()\n",
    "# adf, pvalue = adfuller((np.random.random(1000)-.5).cumsum())[:2]\n",
    "# print(adf, pvalue)\n",
    "# kpss_stat, pvalue = kpss(gen, nlags='auto')[:2]\n",
    "# print(kpss_stat, pvalue)\n",
    "# plt.plot(np.arange(len(gen)), gen)\n",
    "# plt.show()\n",
    "\n",
    "#resstore = adfuller(series, store=True)[-1]\n",
    "#resstore.H0\n",
    "\n",
    "# for k,v in initial_cdfs.items():\n",
    "#     print(\"%s: %s\"%(df_name_id[df_name_id.id == k].descriptor.tolist(), v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#cdf_id = df_name_id.id.unique()[8]\n",
    "#total_counts = np.zeros(16)\n",
    "# for x in symbol_data.values():\n",
    "#     arr = x.get(cdf_id)\n",
    "#     if arr is not None:\n",
    "#         (indices, counts) = np.unique(arr, return_counts=True)\n",
    "#         total_counts[indices] += counts\n",
    "\n",
    "# (indices, counts) = np.unique(x, return_counts=True)\n",
    "# total_counts[indices] += counts\n",
    "# print(total_counts)\n",
    "#\n",
    "# import scipy.stats\n",
    "# scipy.stats.entropy(total_counts, base=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "```c\n",
    "842|static int read_mv_component(aom_reader *r, nmv_component *mvcomp,\n",
    "843|                             int use_subpel, int usehp) {\n",
    "844|  int mag, d, fr, hp;\n",
    "845|  const int sign = aom_read_symbol(r, mvcomp->sign_cdf, 2, ACCT_STR);\n",
    "846|  const int mv_class =\n",
    "847|      aom_read_symbol(r, mvcomp->classes_cdf, MV_CLASSES, ACCT_STR);\n",
    "848|  const int class0 = mv_class == MV_CLASS_0;\n",
    "849|\n",
    "850|  // Integer part\n",
    "851|  if (class0) {\n",
    "852|    d = aom_read_symbol(r, mvcomp->class0_cdf, CLASS0_SIZE, ACCT_STR);\n",
    "853|    mag = 0;\n",
    "854|  } else {\n",
    "855|    const int n = mv_class + CLASS0_BITS - 1;  // number of bits\n",
    "856|    d = 0;\n",
    "857|    for (int i = 0; i < n; ++i)\n",
    "858|      d |= aom_read_symbol(r, mvcomp->bits_cdf[i], 2, ACCT_STR) << i;\n",
    "859|    mag = CLASS0_SIZE << (mv_class + 2);\n",
    "860|  }\n",
    "861|\n",
    "862|  if (use_subpel) {\n",
    "863|    // Fractional part\n",
    "864|    fr = aom_read_symbol(r, class0 ? mvcomp->class0_fp_cdf[d] : mvcomp->fp_cdf,\n",
    "865|                         MV_FP_SIZE, ACCT_STR);\n",
    "866|\n",
    "867|    // High precision part (if hp is not used, the default value of the hp is 1)\n",
    "868|    hp = usehp ? aom_read_symbol(\n",
    "869|                     r, class0 ? mvcomp->class0_hp_cdf : mvcomp->hp_cdf, 2,\n",
    "870|                     ACCT_STR)\n",
    "871|               : 1;\n",
    "872|  } else {\n",
    "873|    fr = 3;\n",
    "874|    hp = 1;\n",
    "875|  }\n",
    "876|\n",
    "877|  // Result\n",
    "878|  mag += ((d << 3) | (fr << 1) | hp) + 1;\n",
    "879|  return sign ? -mag : mag;\n",
    "880|}\n",
    "881|\n",
    "882|static INLINE void read_mv(aom_reader *r, MV *mv, const MV *ref,\n",
    "883|                           nmv_context *ctx, MvSubpelPrecision precision) {\n",
    "884|  MV diff = kZeroMv;\n",
    "885|  const MV_JOINT_TYPE joint_type =\n",
    "886|      (MV_JOINT_TYPE)aom_read_symbol(r, ctx->joints_cdf, MV_JOINTS, ACCT_STR);\n",
    "887|\n",
    "888|  if (mv_joint_vertical(joint_type))\n",
    "889|    diff.row = read_mv_component(r, &ctx->comps[0], precision > MV_SUBPEL_NONE,\n",
    "890|                                 precision > MV_SUBPEL_LOW_PRECISION);\n",
    "891|\n",
    "892|  if (mv_joint_horizontal(joint_type))\n",
    "893|    diff.col = read_mv_component(r, &ctx->comps[1], precision > MV_SUBPEL_NONE,\n",
    "894|                                 precision > MV_SUBPEL_LOW_PRECISION);\n",
    "895|\n",
    "896|  mv->row = ref->row + diff.row;\n",
    "897|  mv->col = ref->col + diff.col;\n",
    "898|}\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}